{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f7fe8ac",
      "metadata": {},
      "source": [
        "# Project 3: **Ask\u2011the\u2011Web Agent**\n",
        "\n",
        "Welcome to Project\u202f3! In this project, you will learn how to use tool\u2011calling LLMs, extend them with custom tools, and build a simplified *Perplexity\u2011style* agent that answers questions by searching the web."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4311a6",
      "metadata": {},
      "source": [
        "## Learning Objectives  \n",
        "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
        "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
        "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
        "* Use **LangChain** to get function\u2011calling capability for free (ReAct reasoning, memory, multi\u2011step planning).\n",
        "* Combine LLM with a web\u2011search tool to build a simple ask\u2011the\u2011web agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f16864",
      "metadata": {},
      "source": [
        "## Roadmap\n",
        "1. Environment setup\n",
        "2. Write simple tools and connect them to an LLM\n",
        "3. Standardize tool calling by writing `to_schema`\n",
        "4. Use LangChain to augment an LLM with your tools\n",
        "5. Build a Perplexity\u2011style web\u2011search agent\n",
        "6. (Optional) A minimal backend and frontend UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeae51d0",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "source": [
        "# 1- Environment setup\n",
        "\n",
        "## 1.1- Conda environment\n",
        "\n",
        "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
        "\n",
        "```bash\n",
        "# Create and activate the conda environment\n",
        "conda env create -f environment.yml && conda activate web_agent\n",
        "\n",
        "# Register this environment as a Jupyter kernel\n",
        "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
        "```\n",
        "Once this is done, you can select \u201cweb_agent\u201d from the Kernel \u2192 Change Kernel menu in Jupyter or VS Code.\n",
        "\n",
        "\n",
        "> Behind the scenes:\n",
        "> * Conda reads `environment.yml`, resolves the pinned dependencies, creates an isolated environment named `web_agent`, and activates it.\n",
        "> * `ollama pull` downloads the model so you can run it locally without API calls.\n",
        "\n",
        "\n",
        "## 1.2 Ollama setup\n",
        "\n",
        "In this project, we start with `gemma3-1B` because it is lightweight and runs on most machines. You can try other smaller or larger LLMs such as `mistral:7b`, `phi3:mini`, or `llama3.2:1b` to compare performance. Explore available models here: https://ollama.com/library\n",
        "\n",
        "```bash\n",
        "ollama pull gemma3:1b\n",
        "```\n",
        "\n",
        "`ollama pull` downloads the model so you can run it locally without API calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c27158f",
      "metadata": {},
      "source": [
        "## 2- Tool\u00a0Calling\n",
        "\n",
        "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
        "\n",
        "\n",
        "We describe each tool\u2019s interface in the model\u2019s prompt, defining what it does and what arguments it expects. When the model decides that a tool is needed, it emits a structured output like: `TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Francisco\"}}`. Your code will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
        "\n",
        "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3` model how to use it when required in four steps:\n",
        "1. Implement the tool\n",
        "2. Create the instructions for the LLM\n",
        "3. Call the LLM with the prompt\n",
        "4. Parse the LLM output and call the tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b5eeca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a536f58",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
        "    \"\"\"Return a simple, human-readable description of the weather.\"\"\"\n",
        "    normalized_city = city.strip()\n",
        "    if not normalized_city:\n",
        "        raise ValueError(\"city must be provided\")\n",
        "\n",
        "    normalized_unit = unit.lower()\n",
        "    if normalized_unit not in {\"celsius\", \"fahrenheit\"}:\n",
        "        normalized_unit = \"celsius\"\n",
        "\n",
        "    temperatures = {\"celsius\": \"22\u00b0C\", \"fahrenheit\": \"72\u00b0F\"}\n",
        "    description = \"partly cloudy\"\n",
        "    return f\"It is {temperatures[normalized_unit]} and {description} in {normalized_city}.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a43c3e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a weather assistant. You can call a single tool named `get_current_weather`\\n\"\n",
        "    \"whenever the user needs up-to-date weather information. The tool accepts a city\\n\"\n",
        "    \"(string) and an optional unit (either 'celsius' or 'fahrenheit'). When the tool\\n\"\n",
        "    \"is required, respond with a single line formatted exactly as:\\n\"\n",
        "    \"TOOL_CALL:{\\\\\"name\\\\\": \\\\\"get_current_weather\\\\\", \\\\\"args\\\\\": {\\\\\"city\\\\\": \\\\\"<city>\\\\\", \\\\\"unit\\\\\": \\\\\"<unit>\\\\\"}}\\n\"\n",
        "    \"If the question can be answered without the tool, reply normally.\"\n",
        ")\n",
        "\n",
        "USER_QUESTION = \"What is the weather in San Diego today?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eebb062",
      "metadata": {},
      "source": [
        "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt.\n",
        "\n",
        "Start the **Ollama** server in a terminal with `ollama serve`. This launches a local API endpoint that listens for LLM requests. Once the server is running, return to the notebook and in the next cell send a query to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027cb75c",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gemma3:1b\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": USER_QUESTION},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response)\n",
        "model_output = response.choices[0].message.content or \"\"\n",
        "print(model_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94aeb4d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, json\n",
        "\n",
        "match = re.search(r\"TOOL_CALL:\\s*(\\{.*\\})\", model_output, flags=re.DOTALL)\n",
        "if not match:\n",
        "    print(\"No tool call requested by the model.\")\n",
        "else:\n",
        "    payload = json.loads(match.group(1))\n",
        "    tool_name = payload.get(\"name\")\n",
        "    args = payload.get(\"args\", {})\n",
        "    print(f\"Calling tool `{tool_name}` with args {args}\")\n",
        "\n",
        "    if tool_name == \"get_current_weather\":\n",
        "        result = get_current_weather(**args)\n",
        "        print(\"Result:\", result)\n",
        "    else:\n",
        "        print(f\"No implementation available for tool `{tool_name}`.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be26661",
      "metadata": {},
      "source": [
        "# 3- Standadize tool calling\n",
        "\n",
        "So far, we handled tool calling manually by writing one regex and one hard-coded function. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the `TOOL_SPEC` prompt.\n",
        "\n",
        "To make the system flexible, we can standardize tool definitions by automatically reading each function\u2019s signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
        "\n",
        "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce911b28",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import inspect\n",
        "\n",
        "\n",
        "def to_schema(fn):\n",
        "    \"\"\"Generate a JSON schema for an arbitrary callable.\"\"\"\n",
        "    signature = inspect.signature(fn)\n",
        "    description = inspect.getdoc(fn) or \"No description provided.\"\n",
        "\n",
        "    type_mapping = {\n",
        "        str: \"string\",\n",
        "        int: \"integer\",\n",
        "        float: \"number\",\n",
        "        bool: \"boolean\",\n",
        "    }\n",
        "\n",
        "    properties = {}\n",
        "    required = []\n",
        "    for name, param in signature.parameters.items():\n",
        "        if param.kind not in (\n",
        "            inspect.Parameter.POSITIONAL_ONLY,\n",
        "            inspect.Parameter.POSITIONAL_OR_KEYWORD,\n",
        "            inspect.Parameter.KEYWORD_ONLY,\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        annotation = param.annotation if param.annotation is not inspect._empty else str\n",
        "        schema_type = type_mapping.get(annotation, \"string\")\n",
        "        properties[name] = {\n",
        "            \"type\": schema_type,\n",
        "            \"description\": f\"Argument `{name}` for {fn.__name__}.\",\n",
        "        }\n",
        "        if param.default is inspect._empty:\n",
        "            required.append(name)\n",
        "\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": fn.__name__,\n",
        "            \"description\": description,\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": properties,\n",
        "                \"required\": required,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "tool_schema = to_schema(get_current_weather)\n",
        "pprint(tool_schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1163f18",
      "metadata": {},
      "outputs": [],
      "source": [
        "messages_with_schema = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"tool_spec\",\n",
        "        \"content\": json.dumps({\"tools\": [tool_schema]}),\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": USER_QUESTION},\n",
        "]\n",
        "\n",
        "response_with_schema = client.chat.completions.create(\n",
        "    model=\"gemma3:1b\",\n",
        "    messages=messages_with_schema,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "schema_model_output = response_with_schema.choices[0].message.content or \"\"\n",
        "print(schema_model_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8ec86e",
      "metadata": {},
      "source": [
        "## 4-\u202fLangChain for Tool Calling\n",
        "So far, you built a simple tool-calling pipeline manually. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning.\n",
        "\n",
        "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward.\n",
        "\n",
        "In this section, you will use the **ReAct** Agent (Reasoning + Acting). It alternates between reasoning steps and tool use, producing clearer and more reliable results. We will explore reasoning-focused models in more depth next week.\n",
        "\n",
        "The following links might be helpful:\n",
        "- https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html\n",
        "- https://python.langchain.com/docs/integrations/tools/\n",
        "- https://python.langchain.com/docs/integrations/chat/ollama/\n",
        "- https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.LLM.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c609d97c",
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_weather(city: str, unit: str = \"celsius\") -> str:\n",
        "    \"\"\"Return a natural language weather report for a city.\"\"\"\n",
        "    return get_current_weather(city=city, unit=unit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9552348d",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
        "\n",
        "langchain_tools = [get_weather]\n",
        "weather_agent = initialize_agent(\n",
        "    langchain_tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "agent_result = weather_agent.invoke({\"input\": \"Do I need an umbrella in Seattle today?\"})\n",
        "print(agent_result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed9e8fb",
      "metadata": {},
      "source": [
        "### What just happened?\n",
        "\n",
        "The console log displays the **Thought\u202f\u2192\u202fAction\u202f\u2192\u202fObservation\u202f\u2192\u202f\u2026** loop until the agent produces its final answer. Because `verbose=True`, LangChain prints each intermediate reasoning step.\n",
        "\n",
        "If you want to add more tools, simply append them to the tools list. LangChain will handle argument validation, schema generation, and tool-calling logic automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 5- Perplexity\u2011Style Web Search\n",
        "Agents become much more powerful when they can look up real information on the web instead of relying only on their internal knowledge.\n",
        "\n",
        "In this section, you will combine everything you have learned to build a simple Ask-the-Web Agent. You will integrate a web search tool (DuckDuckGo) and make it available to the agent using the same tool-calling approach as before.\n",
        "\n",
        "This will let the model retrieve fresh results, reason over them, and generate an informed answer\u2014similar to how Perplexity works.\n",
        "\n",
        "You may find some examples from the following links:\n",
        "- https://pypi.org/project/duckduckgo-search/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Search DuckDuckGo for the query and return a summary of the top results.\"\"\"\n",
        "    results = []\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            for idx, entry in enumerate(ddgs.text(query, max_results=3), start=1):\n",
        "                title = entry.get(\"title\") or \"Untitled result\"\n",
        "                snippet = entry.get(\"body\") or \"\"\n",
        "                url = entry.get(\"href\") or entry.get(\"url\") or \"\"\n",
        "                summary = f\"{idx}. {title}\\n{snippet}\".strip()\n",
        "                if url:\n",
        "                    summary += f\"\\n{url}\"\n",
        "                results.append(summary)\n",
        "    except Exception as exc:\n",
        "        return f\"Web search failed: {exc}\"\n",
        "\n",
        "    if not results:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    return \"\\n\\n\".join(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "web_llm = OpenAI(\n",
        "    temperature=0,\n",
        "    model_name=\"gemma3:1b\",\n",
        "    openai_api_key=\"ollama\",\n",
        "    openai_api_base=\"http://localhost:11434/v1\",\n",
        ")\n",
        "\n",
        "web_tools = [web_search]\n",
        "web_agent = initialize_agent(\n",
        "    web_tools,\n",
        "    web_llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let\u2019s see the agent's output in action with a real example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What are the current events in San Francisco this week?\"\n",
        "web_response = web_agent.invoke({\"input\": question})\n",
        "print(web_response[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 6- A minimal UI\n",
        "This project includes a simple **React** front end that sends the user\u2019s question to a FastAPI back end and streams the agent\u2019s response in real time. To run the UI:\n",
        "\n",
        "1- Open a terminal and start the Ollama server: `ollama serve`.\n",
        "\n",
        "2- In a second terminal, navigate to the frontend folder and install dependencies:`npm install`.\n",
        "\n",
        "3- In the same terminal, start the FastAPI back\u2011end: `uvicorn app:app --reload --port 8000`\n",
        "\n",
        "4- Open a third terminal, stay in the frontend folder, and start the React dev server: `npm run dev`\n",
        "\n",
        "5- Visit `http://localhost:5173/` in your browser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \ud83c\udf89 Congratulations!\n",
        "\n",
        "* You have built a **web\u2011enabled agent**: tool calling \u2192 JSON schema \u2192 LangChain ReAct \u2192 web search \u2192 simple UI.\n",
        "* Try adding more tools, such as news or finance APIs.\n",
        "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
        "\n",
        "\n",
        "\ud83d\udc4f **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "web_agent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}